{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poojasri1726/fmml_lab/blob/main/Copy_of_FMML_Module_2_project_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: Appreciating, Interpreting and Visualizing Data\n",
        "## Project\n",
        "\n",
        "```\n",
        "Coordinator: Aswin Jose\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "R976BTHqth_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "PjKikpD7uCnb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we wil be performing a simple Exploratory Data Anaysis for this project. We will use the methods we learned in the tutorials to have a basic understanding of the dataset. So first we will start with the heart dataset available from kaggle. the infomration about the columns of the dataset is given below:    \n",
        "-age    \n",
        "-sex    \n",
        "-chest pain type (4 values)    \n",
        "-resting blood pressure    \n",
        "-serum cholestoral in mg/dl    \n",
        "-fasting blood sugar > 120 mg/dl    \n",
        "-resting electrocardiographic results (values 0,1,2)    \n",
        "-maximum heart rate achieved    \n",
        "-exercise induced angina   \n",
        "-oldpeak = ST depression induced by exercise relative to rest    \n",
        "-the slope of the peak exercise ST segment    \n",
        "-number of major vessels (0-3) colored by flourosopy    \n",
        "-:thal: 0 = normal; 1 = fixed defect; 2 = reversable defect    "
      ],
      "metadata": {
        "id": "Ep9v4Rj1rDaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the portions that says \"to do\""
      ],
      "metadata": {
        "id": "GVPNEAfBrs7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded1 = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "I9sX_JSdtpsH",
        "outputId": "b942b021-0854-4355-993d-f05920fd1906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-12ca30bb-e118-46b3-bb7f-498822ecd345\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-12ca30bb-e118-46b3-bb7f-498822ecd345\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"heart.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "7CUciJW-t-aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "j3fHpCWfuWSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns\n"
      ],
      "metadata": {
        "id": "DuxMngbivcdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "N1OCngCVr5H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## we will be comparing rest of the parameters/columns present in the data with respect to precence or absece of heart disease\n",
        "data['target'] = data.target.replace({1: \"Disease\", 0: \"No_disease\"})\n",
        "data['sex'] = data.sex.replace({1: \"Male\", 0: \"Female\"})\n",
        "data['cp'] = data.cp.replace({1: \"typical_angina\",\n",
        "                          2: \"atypical_angina\",\n",
        "                          3:\"non-anginal pain\",\n",
        "                          4: \"asymtomatic\"})\n",
        "data['exang'] = data.exang.replace({1: \"Yes\", 0: \"No\"})\n",
        "data['slope'] = data.cp.replace({1: \"upsloping\",\n",
        "                          2: \"flat\",\n",
        "                          3:\"downsloping\"})\n",
        "data['thal'] = data.thal.replace({1: \"fixed_defect\", 2: \"reversable_defect\", 3:\"normal\"})"
      ],
      "metadata": {
        "id": "fRHrjko7xIwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "wl5oM9BZ2XVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets look at the difference in the number of samples with and without disease using a barplot."
      ],
      "metadata": {
        "id": "RKjU-NUW3op-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data['target'].value_counts())\n",
        "plt.title('Heart Disease Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9H3aQvCe2prB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we can plot the same barplots usng the pandas inbuilt plotting functions.\n",
        "data['target'].value_counts().plot(kind='bar').set_title('Heart Disease Classes')"
      ],
      "metadata": {
        "id": "UpL7yRHR2ZA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pie charts can also be used to show the same infomation in a different manner\n",
        "plt.pie(data['target'].value_counts(), labels=[\"Disease\", \"No disease\"], autopct='%1.1f%%')\n",
        "plt.title('Target Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6lzeMeXx6Ddl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next we will plot the counts of all the non-continous features present in the dataset.\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    sns.barplot(data[feature].value_counts(), ax=ax)"
      ],
      "metadata": {
        "id": "lhrwKDM62l67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  now lets play with 2 vaiables in dataset. Lets see if chest pain translates to the presence of desease in most cases...\n",
        "sns.countplot(x='cp', hue='target', data=data, palette='rainbow').set_title('Disease classes according to Chest Pain')"
      ],
      "metadata": {
        "id": "5MNCxVhQ4x30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets visualise count of all vairables w.r.t the presence of disease togather:\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    ## to do\n"
      ],
      "metadata": {
        "id": "smGhn01t5bqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the distribution of the continous variables"
      ],
      "metadata": {
        "id": "V0CiDMIK6pCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pair plots can automoaticaly be used to viwe the pairwise relationship between all the  feature that we selected\n",
        "continous_features = ['age', 'chol', 'thalach', 'oldpeak','trestbps']\n",
        "sns.pairplot(data[continous_features + ['target']], hue='target')"
      ],
      "metadata": {
        "id": "6KnGCZz632hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets try to understand the relationship between age and chol in each of the target based on sex.\n",
        "sns.lmplot(x=\"age\", y=\"chol\", hue=\"sex\", col=\"target\",\n",
        "           palette=\"Set1\",\n",
        "           data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DtjSCsgl6vV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = data[continous_features]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numeric_data.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ywPbmTHx8Rqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, len(continous_features), figsize=(25, 4), sharex=False, sharey=False)\n",
        "\n",
        "for idx, feature in enumerate(continous_features):\n",
        "    sns.boxplot(x='target', y=feature, data=data, ax=axes[idx])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOQvj2UT-4Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot the cumulative variace of pca for all the possibel pronviopal components\n",
        "## to do\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca = PCA()\n",
        "pca.fit(numeric_data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QLy3U9yOAAa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(numeric_data)\n",
        "pca_data = pca.transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the principal components and target labels\n",
        "pca_df = pd.DataFrame({\n",
        "    \"pca_1\": pca_data[:, 0],\n",
        "    \"pca_2\": pca_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the PCA results with a scatter plot\n",
        "sns.scatterplot(x=\"pca_1\", y=\"pca_2\", hue=\"target\", data=pca_df)\n",
        "plt.title(\"PCA Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9kb_TztC9hqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Initialize and fit the TSNE model\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_data = tsne.fit_transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the TSNE components and target labels\n",
        "tsne_df = pd.DataFrame({\n",
        "    \"tsne_1\": tsne_data[:, 0],\n",
        "    \"tsne_2\": tsne_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the TSNE results with a scatter plot\n",
        "sns.scatterplot(x=\"tsne_1\", y=\"tsne_2\", hue=\"target\", data=tsne_df)\n",
        "plt.title(\"TSNE Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QDKt3GQIAMHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the plots above, answer the following questions:    \n",
        "1. What is the percentage of Samples with Disease?    \n",
        "2. what are the 3 continous features that shows a singnficanct statistical differnce in distribution with respect to the precence and absence of the disease?    \n",
        "3. Can we see a clear seperation in terms of the presence/absence of disease in the features obtained from pca and tsne plots?    \n",
        "4. What is the optimal number of principal components in our case?    \n",
        "5. what are the continous features with the highest correation with each other?"
      ],
      "metadata": {
        "id": "AoQ8InmSuetP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1:-**\n",
        "           The percentage of samples with disease is 54.5% is taken from the pie chart of heart disease sample. It was directly showing the percentage remaining bar graphs or others are showing target variables."
      ],
      "metadata": {
        "id": "3KC8ohQavzBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 2:-**\n",
        "\n",
        "thalach (maximum heart rate achieved), oldpeak (ST depression induced by exercise relative to rest), and chol(serum cholestoral in mg/dl) show significant statistical differences in distribution between the presence and absence of disease.by seeing the continuous features for the disease and no disease groups."
      ],
      "metadata": {
        "id": "T0Gjnrf9xdeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3:-**\n",
        "\n",
        "While there is some degree of clustering in the PCA and t-SNE plots, a complete and clear separation between the two target groups is not evident. There is still some overlap."
      ],
      "metadata": {
        "id": "C5WYlRzIylmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 4:-**\n",
        "\n",
        "The optimal number of principal components can be determined by examining the cumulative explained variance plot generated by the following code:"
      ],
      "metadata": {
        "id": "NEQlKv0KzoiG"
      }
    },
    {
      "source": [
        "pca = PCA()\n",
        "pca.fit(numeric_data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BbN47eUyzy5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 5:-**\n",
        "\n",
        "The continuous features with the highest correlation are:\n",
        "\n",
        "age and trestbps (resting blood pressure): They have a correlation coefficient of approximately 0.28, indicating a moderate positive correlation. This suggests that as age increases, resting blood pressure tends to increase as well.\n",
        "oldpeak (ST depression) and thalach (maximum heart rate): They have a correlation coefficient of approximately -0.34, indicating a moderate negative correlation. This suggests that as ST depression increases, maximum heart rate tends to decrease.\n",
        "These two pairs of continuous features exhibit the strongest correlations within the dataset. It is important to note that correlation does not imply causation, but it highlights potential relationships between these variables that might be worth further investigation."
      ],
      "metadata": {
        "id": "LgxI0PuLz1XM"
      }
    },
    {
      "source": [
        "numeric_data = data[continous_features]\n",
        "corr_matrix = numeric_data.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6MM0OBPl0I-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets move on to do the same analysis on the starbucks nutrition dataset. this dataset contains the nutrition information of starbucks drinks."
      ],
      "metadata": {
        "id": "w8hH8CAhv8tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upload2 = files.upload()"
      ],
      "metadata": {
        "id": "a_RrGpN1ApCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"star_nutri_expanded.csv\")"
      ],
      "metadata": {
        "id": "NImD-yk0fpmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "YXZJPk4Zf636"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning and filling the missing values in the data"
      ],
      "metadata": {
        "id": "2JA9ZVyywXk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].replace('Varies', np.NaN).replace('varies', np.NaN)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].astype(np.float64)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].fillna(data['Caffeine (mg)'].mean())"
      ],
      "metadata": {
        "id": "VrE3QA0GgCTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'].unique()"
      ],
      "metadata": {
        "id": "0PNMhN2GiQ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'] = data['Total Fat (g)'].replace('3 2', '3.2')"
      ],
      "metadata": {
        "id": "Suua5EkTiRxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "u-I6TsTLkiMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract columns with int and float types\n",
        "numeric_columns = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Print the numeric columns\n",
        "print(numeric_columns)\n"
      ],
      "metadata": {
        "id": "xPfMl26LknBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be analysing the dataset using the fact that weather the drink comes under the category tea or not"
      ],
      "metadata": {
        "id": "9FY9pzBNwete"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Beverage_category'].unique()"
      ],
      "metadata": {
        "id": "LVljNoJgg9Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Tea'] = data['Beverage_category'].apply(lambda x: 1 if x == 'Tazo® Tea Drinks' else 0)\n",
        "data = data.drop('Beverage_category', axis=1)"
      ],
      "metadata": {
        "id": "4UZ4Tk7whVEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  one hot encoding of categorical features in data\n",
        "def onehot_encode(df, columns, prefixes):\n",
        "    df = df.copy()\n",
        "    for column, prefix in zip(columns, prefixes):\n",
        "        dummies = pd.get_dummies(df[column], prefix=prefix)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df = df.drop(column, axis=1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "OZanK3hSi6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = onehot_encode(\n",
        "    data,\n",
        "    columns=['Beverage', 'Beverage_prep'],\n",
        "    prefixes=['bev', 'bevp']\n",
        ")"
      ],
      "metadata": {
        "id": "PlZCj2rwi7us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = data.replace({True: 1, False: 0})\n"
      ],
      "metadata": {
        "id": "w7qKp8JMwysG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.applymap(lambda x: np.float64(str(x).replace('%', '')))"
      ],
      "metadata": {
        "id": "I-RIoXq6ig3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "J_BgqsKogWE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "iB5PmiPojj3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "\n",
        "# Create a pie chart of the 'Tea' column also write your observation form the plot\n",
        "\n",
        "tea_counts = data['Tea'].value_counts()\n",
        "\n",
        "\n",
        "plt.pie(tea_counts, labels=['Non-Tea', 'Tea'], autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Proportion of Tea and Non-Tea Drinks')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KxZYShn1hpYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "\n",
        "The pie chart visually represents the proportion of tea and non-tea drinks in the Starbucks dataset. Based on the generated plot, we can observe the following:\n",
        "\n",
        "Majority of drinks are Non-Tea: The largest slice of the pie represents Non-Tea drinks, indicating that a larger portion of Starbucks beverages do not fall under the \"Tazo® Tea Drinks\" category.\n",
        "Smaller proportion of Tea drinks: The smaller slice represents Tea drinks, suggesting that Tazo® Tea Drinks constitute a smaller percentage of the overall Starbucks beverage offerings.\n",
        "By visualizing the distribution of the 'Tea' column using a pie chart, we can quickly grasp the relative proportions of tea and non-tea drinks within the dataset. This information can be valuable for understanding the overall composition of Starbucks' beverage offerings."
      ],
      "metadata": {
        "id": "6VeYq0vF1GMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# perform pca on the data and plot the explained variace ratio, what is the optimal number of principal components in this case ?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame containing the Starbucks nutrition data\n",
        "\n",
        "# Select only numeric features for PCA\n",
        "numeric_data = data.select_dtypes(include=np.number)\n",
        "\n",
        "# Initialize PCA and fit to the data\n",
        "pca = PCA()\n",
        "pca.fit(numeric_data)\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot the explained variance ratio\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.title('Explained Variance Ratio vs. Number of Principal Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HL2dDjYfh2L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame containing the Starbucks nutrition data\n",
        "\n",
        "# Select only numeric features for PCA\n",
        "numeric_data = data.select_dtypes(include=np.number)\n",
        "\n",
        "# Initialize PCA and fit to the data\n",
        "pca = PCA()\n",
        "pca.fit(numeric_data)\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot the explained variance ratio\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.title('Explained Variance Ratio vs. Number of Principal Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6h8VwK9-2OKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# visualise the principal components, choose the number of principal components based on the above plot. What is you observation from the plot?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data' is your DataFrame containing the Starbucks nutrition data\n",
        "# and 'pca' is the fitted PCA object from the previous step\n",
        "\n",
        "# Choose the number of principal components based on the explained variance plot\n",
        "# For example, if you want to retain 95% of the variance, you can find\n",
        "# the number of components where the cumulative explained variance ratio\n",
        "# first exceeds 0.95.\n",
        "n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
        "\n",
        "# Transform the data to the principal component space\n",
        "pca_data = pca.transform(numeric_data)[:, :n_components]\n",
        "\n",
        "# Create a DataFrame with the principal components and target labels\n",
        "pca_df = pd.DataFrame(pca_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
        "pca_df[\"Tea\"] = data[\"Tea\"]  # Assuming 'Tea' is your target variable\n",
        "\n",
        "# Visualize the principal components with a scatter plot\n",
        "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Tea\", data=pca_df)\n",
        "plt.title(\"PCA Visualization of Starbucks Nutrition Data\")\n",
        "plt.xlabel(f\"Principal Component 1 (Explains {pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
        "plt.ylabel(f\"Principal Component 2 (Explains {pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tNKX2JJVjwrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning:\n",
        "\n",
        "Determine the optimal number of components: We use the cumulative_variance_ratio and np.argmax to find the index where the cumulative variance first exceeds a desired threshold (e.g., 0.95). Adding 1 to this index gives us the optimal number of components to retain.\n",
        "Transform the data: pca_data = pca.transform(numeric_data)[:, :n_components] transforms the original numeric data into the principal component space, keeping only the selected number of components.\n",
        "Create a DataFrame: A new DataFrame (pca_df) is created with the principal components and the target variable (\"Tea\" in this case).\n",
        "Visualize with scatter plot: We use seaborn.scatterplot to create a scatter plot of the first two principal components, color-coded by the target variable. This visualization helps to see if there is any separation between the classes (tea and non-tea drinks) in the lower-dimensional space.\n",
        "Observation:\n",
        "\n",
        "By examining the PCA visualization, you can observe how the data points cluster or separate based on the \"Tea\" category. If there is a clear separation between tea and non-tea drinks in the plot, it suggests that the principal components capture the essential features that distinguish these categories. You can also look for patterns or relationships between the principal components and other variables in the dataset.\n",
        "\n",
        "Additionally, the explained variance ratio for each principal component, indicated in the axis labels, tells you how much of the total variance in the data is explained by that component. This information helps to understand the importance of each component in representing the data's variability."
      ],
      "metadata": {
        "id": "6yfUEgIP2d04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# plot the first 2 components of tsne, whats you observation from the plot?\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'numeric_data' is your DataFrame containing the Starbucks nutrition data\n",
        "\n",
        "# Initialize and fit t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)  # Set random_state for reproducibility\n",
        "tsne_data = tsne.fit_transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the t-SNE components and target labels\n",
        "tsne_df = pd.DataFrame(tsne_data, columns=[\"t-SNE 1\", \"t-SNE 2\"])\n",
        "tsne_df[\"Tea\"] = data[\"Tea\"]  # Assuming 'Tea' is your target variable\n",
        "\n",
        "# Visualize the t-SNE components with a scatter plot\n",
        "sns.scatterplot(x=\"t-SNE 1\", y=\"t-SNE 2\", hue=\"Tea\", data=tsne_df)\n",
        "plt.title(\"t-SNE Visualization of Starbucks Nutrition Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tgpIgiYdlVcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "\n",
        "By examining the t-SNE plot, you can observe how the data points cluster or separate based on the \"Tea\" category. If there is a clear separation between tea and non-tea drinks in the plot, it suggests that t-SNE has effectively captured the underlying structure of the data and revealed distinct clusters based on the target variable.\n",
        "\n",
        "t-SNE focuses on preserving local neighborhood relationships in the data, which can be helpful for visualizing complex datasets and identifying potential clusters or patterns. However, it's important to note that the distances between clusters in a t-SNE plot might not accurately reflect the true distances in the original high-dimensional space.\n",
        "\n",
        "Compare the t-SNE plot with the PCA plot. If both methods show similar patterns of clustering or separation, it strengthens the evidence for the presence of distinct groups in the data. If the patterns differ, it might indicate that the methods are capturing different aspects of the data structure, which could be further investigated."
      ],
      "metadata": {
        "id": "uE7H_GEE246D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# create a correlation matrix and plot the heatmap, whats your observation from the heatmap ?\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame containing the Starbucks nutrition data\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))  # Adjust figure size as needed\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix Heatmap of Starbucks Nutrition Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cOWaPpnmj4OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "\n",
        "By examining the heatmap, you can observe the following:\n",
        "\n",
        "Strong positive correlations: Dark red squares indicate strong positive correlations between variables. For example, \"Calories\" might have a strong positive correlation with \"Total Fat (g)\" and \"Sugars (g)\".\n",
        "Strong negative correlations: Dark blue squares indicate strong negative correlations. You might observe negative correlations between variables like \"Fiber (g)\" and \"Sugars (g)\".\n",
        "Weak or no correlations: Lighter colors or white squares indicate weak or no correlations.\n",
        "Look for patterns and relationships between variables. For example, you might find that certain beverage categories have higher correlations with specific nutritional components. This information can provide insights into the nutritional profiles of different Starbucks drinks and help identify potential health implications.\n",
        "\n",
        "Focus on the correlations related to the \"Tea\" variable to understand its relationships with other nutritional factors. This might reveal specific features that are strongly associated with tea drinks compared to non-tea drinks.\n",
        "\n",
        "Consider the magnitude and direction of correlations to draw meaningful conclusions. For example, a correlation coefficient of 0.8 might indicate a strong positive relationship, while a coefficient of -0.6 might suggest a moderate negative relationship.\n",
        "\n",
        "Remember that correlation does not imply causation. Even if two variables are strongly correlated, it doesn't necessarily mean that one causes the other. Further investigation is needed to establish causal relationships."
      ],
      "metadata": {
        "id": "B8xILQu23MsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# make a boxplot of all the numeric columns of the dataset. Which column/columns can be the most potential indicator weather its a tea or a non tea drink?\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame containing the Starbucks nutrition data\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_columns = data.select_dtypes(include=['number'])\n",
        "\n",
        "# Create box plots for each numeric column\n",
        "for column in numeric_columns:\n",
        "    plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "    sns.boxplot(x='Tea', y=column, data=data)\n",
        "    plt.title(f'Box Plot of {column} by Tea Category')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VJMRR3yqkNb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries: pandas, seaborn, and matplotlib.pyplot are imported for data manipulation, box plot creation, and general plotting.\n",
        "Select numeric columns: numeric_columns = data.select_dtypes(include=['number']) selects all columns with numeric data types.\n",
        "Create box plots: The code iterates through each numeric column and creates a box plot using sns.boxplot. The x argument is set to 'Tea' to group the data by tea category, and the y argument is set to the current numeric column.\n",
        "Potential Indicators:\n",
        "\n",
        "To identify potential indicators for tea or non-tea drinks, examine the box plots and look for columns where the distributions for tea and non-tea drinks are significantly different.\n",
        "\n",
        "Here are some factors to consider:\n",
        "\n",
        "Median: If the median values for tea and non-tea drinks are substantially different, it suggests that the column might be a good indicator.\n",
        "Interquartile Range (IQR): A larger difference in the IQR between tea and non-tea drinks indicates greater variability within each category, which can be useful for distinguishing them.\n",
        "Outliers: The presence of outliers can also provide clues. If one category has more outliers than the other, it might suggest that the column is sensitive to differences between tea and non-tea drinks.\n",
        "Specific Columns:\n",
        "\n",
        "Based on the box plots generated by the code, you can analyze the following columns as potential indicators:\n",
        "\n",
        "Calories: Tea drinks generally have lower calorie counts compared to non-tea drinks.\n",
        "Total Fat (g): Tea drinks often have lower total fat content.\n",
        "Sugars (g): Non-tea drinks, especially those with added flavors or syrups, tend to have higher sugar content.\n",
        "Caffeine (mg): While tea naturally contains caffeine, some non-tea drinks might have higher caffeine levels due to added coffee or energy ingredients.\n",
        "Sodium (mg): Some non-tea drinks might contain more sodium than tea drinks.\n",
        "By carefully comparing the box plots for these and other numeric columns, you can identify the most potential indicators for distinguishing tea and non-tea drinks in the Starbucks nutrition dataset. Remember to consider the context and the overall patterns observed in the data."
      ],
      "metadata": {
        "id": "lZ0xidZz3bgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the clarity and professionalism of the provided text, consider the following refined version: In the process of conducting a preliminary Exploratory Data Analysis (EDA), we have utilized various techniques to gain insights into the datasets under consideration. It's important to note that our analysis extends beyond the initial visualizations, embracing a multitude of methods to thoroughly understand the data.\n",
        "Among the array of tools available for EDA, one particularly easy solution is the use of the pandas profiling library. This tool significantly simplifies the process of exploring the fundamental distribution of data within a dataset. By generating detailed profile reports, pandas profiling provides a comprehensive overview of the dataset's characteristics, including but not limited to, the distribution of variables, presence of missing values, and potential correlations between variables.\n",
        "Furthermore, we are utilizing Google Colab notebooks, the integration of AI tools offers an additional avenue for data visualization and analysis. These tools can automatically generate insightful plots and statistics, further enriching the data exploration process."
      ],
      "metadata": {
        "id": "_yRYeXpfzY7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During our preliminary Exploratory Data Analysis (EDA), we employed various techniques to gain insights into the datasets. Our analysis went beyond basic visualizations, incorporating a comprehensive range of methods for a thorough understanding of the data.\n",
        "\n",
        "While numerous EDA tools are available, one particularly convenient solution is the pandas profiling library. This tool significantly simplifies the initial exploration of data distribution within a dataset. By generating detailed profile reports, pandas profiling offers a comprehensive overview of the dataset's characteristics, including but not limited to variable distributions, missing values, and potential correlations.\n",
        "\n",
        "Furthermore, leveraging the integration of AI tools within Google Colab notebooks provides an additional avenue for data visualization and analysis. These tools can automatically generate insightful plots and statistics, further enriching the data exploration process.\"\n",
        "\n",
        "Improvements:\n",
        "\n",
        "Concise Language: The refined version uses more concise language, avoiding unnecessary repetition and redundancy.\n",
        "Formal Tone: The tone is more formal and professional, using appropriate terminology and avoiding casual expressions.\n",
        "Stronger Verbs: Stronger verbs like \"employed\" and \"incorporating\" replace weaker verbs like \"utilized\" and \"embracing\" to convey a more active and confident voice.\n",
        "Clear Structure: The text is organized into clear paragraphs with distinct topics, making it easier to follow and understand.\n",
        "Emphasis on Key Points: The refined version highlights the importance of going beyond basic visualizations and utilizing a comprehensive approach to EDA.\n",
        "Specific Examples: The mention of pandas profiling and AI tools provides concrete examples of the tools and techniques used in the analysis."
      ],
      "metadata": {
        "id": "-Ud3h7Lo30PU"
      }
    }
  ]
}